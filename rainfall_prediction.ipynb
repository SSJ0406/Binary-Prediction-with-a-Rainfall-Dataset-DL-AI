{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84839b11-3b08-46a1-b0b6-2bbf64981541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "=== Advanced Rainfall Prediction Model ===\n",
      "Adapted for local file paths\n",
      "\n",
      "Start time: 2025-04-07 03:24:51.494649\n",
      "\n",
      "[1/5] Loading data...\n",
      "Training data shape: (2190, 13)\n",
      "Test data shape: (730, 12)\n",
      "\n",
      "[2/5] Preprocessing data and creating meteorological features...\n",
      "Starting data preprocessing...\n",
      "Target distribution: rainfall\n",
      "1    75.342466\n",
      "0    24.657534\n",
      "Name: proportion, dtype: float64\n",
      "Creating optimized meteorological features...\n",
      "Creating weather pattern clusters...\n",
      "Creating stratified folds for cross-validation...\n",
      "Applying feature scaling...\n",
      "Preprocessing complete. Features: 38\n",
      "\n",
      "[3/5] Running cross-validation with ensemble of models...\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Train size: 1752, Validation size: 438, Test size: 730\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4343\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's auc: 0.925014\n",
      "LightGBM validation AUC: 0.925014\n",
      "\n",
      "Top 10 important features for LightGBM:\n",
      "                    Feature   Importance\n",
      "20  weighted_cloud_humidity  4125.353930\n",
      "16   cloud_humidity_product  3538.502029\n",
      "33     cloud_sunshine_ratio  2067.906552\n",
      "7                     cloud  1934.206212\n",
      "17     cloud_humidity_ratio   561.397077\n",
      "35                   wind_v   378.642236\n",
      "8                  sunshine   340.192324\n",
      "5                  dewpoint   266.311532\n",
      "13            calc_humidity   142.994730\n",
      "6                  humidity   122.121144\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-auc:0.88252\tval-auc:0.90220\n",
      "[61]\ttrain-auc:0.93549\tval-auc:0.91902\n",
      "XGBoost validation AUC: 0.919024\n",
      "\n",
      "Training k-NN model...\n",
      "k-NN validation AUC: 0.890194\n",
      "Epoch 10/100: Loss: 0.4407, Val AUC: 0.923260\n",
      "Epoch 20/100: Loss: 0.4316, Val AUC: 0.925028\n",
      "Early stopping at epoch 26\n",
      "Epoch 10/100: Loss: 0.4324, Val AUC: 0.923850\n",
      "Epoch 20/100: Loss: 0.4057, Val AUC: 0.920511\n",
      "Epoch 30/100: Loss: 0.4298, Val AUC: 0.922166\n",
      "Early stopping at epoch 37\n",
      "Fold 1 Ensemble AUC: 0.917396\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Train size: 1752, Validation size: 438, Test size: 730\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4328\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.850238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's auc: 0.852862\n",
      "LightGBM validation AUC: 0.852862\n",
      "\n",
      "Top 10 important features for LightGBM:\n",
      "                    Feature    Importance\n",
      "16   cloud_humidity_product  12937.732353\n",
      "20  weighted_cloud_humidity   8602.936052\n",
      "7                     cloud   3315.606108\n",
      "33     cloud_sunshine_ratio   1601.983761\n",
      "17     cloud_humidity_ratio    902.682517\n",
      "35                   wind_v    833.927878\n",
      "8                  sunshine    461.347997\n",
      "4                   mintemp    436.676420\n",
      "19         cloud_normalized    422.781195\n",
      "13            calc_humidity    416.863176\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-auc:0.90783\tval-auc:0.83481\n",
      "[73]\ttrain-auc:0.95013\tval-auc:0.84750\n",
      "XGBoost validation AUC: 0.847503\n",
      "\n",
      "Training k-NN model...\n",
      "k-NN validation AUC: 0.845062\n",
      "Epoch 10/100: Loss: 0.3882, Val AUC: 0.849046\n",
      "Epoch 20/100: Loss: 0.3741, Val AUC: 0.846605\n",
      "Early stopping at epoch 22\n",
      "Epoch 10/100: Loss: 0.3912, Val AUC: 0.846380\n",
      "Epoch 20/100: Loss: 0.3879, Val AUC: 0.845511\n",
      "Early stopping at epoch 21\n",
      "Fold 2 Ensemble AUC: 0.851936\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Train size: 1752, Validation size: 438, Test size: 730\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4333\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.876669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid_0's auc: 0.879279\n",
      "LightGBM validation AUC: 0.879279\n",
      "\n",
      "Top 10 important features for LightGBM:\n",
      "                    Feature    Importance\n",
      "16   cloud_humidity_product  14591.833574\n",
      "20  weighted_cloud_humidity   9527.631766\n",
      "7                     cloud   5870.895503\n",
      "33     cloud_sunshine_ratio   2696.376242\n",
      "19         cloud_normalized   1716.919732\n",
      "35                   wind_v   1158.587186\n",
      "8                  sunshine    993.421690\n",
      "11               temp_range    755.398586\n",
      "17     cloud_humidity_ratio    751.080048\n",
      "2                   maxtemp    653.673135\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-auc:0.88700\tval-auc:0.82072\n",
      "[100]\ttrain-auc:0.94256\tval-auc:0.87323\n",
      "[200]\ttrain-auc:0.95314\tval-auc:0.87464\n",
      "[289]\ttrain-auc:0.96151\tval-auc:0.87464\n",
      "XGBoost validation AUC: 0.874635\n",
      "\n",
      "Training k-NN model...\n",
      "k-NN validation AUC: 0.833558\n",
      "Epoch 10/100: Loss: 0.3962, Val AUC: 0.869220\n",
      "Epoch 20/100: Loss: 0.4239, Val AUC: 0.871212\n",
      "Early stopping at epoch 24\n",
      "Epoch 10/100: Loss: 0.4302, Val AUC: 0.870932\n",
      "Epoch 20/100: Loss: 0.4023, Val AUC: 0.872643\n",
      "Epoch 30/100: Loss: 0.3857, Val AUC: 0.871605\n",
      "Epoch 40/100: Loss: 0.3938, Val AUC: 0.870006\n",
      "Early stopping at epoch 40\n",
      "Fold 3 Ensemble AUC: 0.866751\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Train size: 1752, Validation size: 438, Test size: 730\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4340\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.904826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's auc: 0.905696\n",
      "LightGBM validation AUC: 0.905696\n",
      "\n",
      "Top 10 important features for LightGBM:\n",
      "                    Feature    Importance\n",
      "16   cloud_humidity_product  14344.808888\n",
      "20  weighted_cloud_humidity   7562.440985\n",
      "7                     cloud   4736.874540\n",
      "33     cloud_sunshine_ratio   2052.922409\n",
      "19         cloud_normalized   1231.587791\n",
      "8                  sunshine    777.799956\n",
      "35                   wind_v    710.131186\n",
      "17     cloud_humidity_ratio    613.921762\n",
      "34                   wind_u    518.507667\n",
      "13            calc_humidity    483.135192\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-auc:0.88608\tval-auc:0.86260\n",
      "[100]\ttrain-auc:0.94091\tval-auc:0.90603\n",
      "[200]\ttrain-auc:0.95351\tval-auc:0.90948\n",
      "[288]\ttrain-auc:0.96228\tval-auc:0.91030\n",
      "XGBoost validation AUC: 0.910185\n",
      "\n",
      "Training k-NN model...\n",
      "k-NN validation AUC: 0.868434\n",
      "Epoch 10/100: Loss: 0.4152, Val AUC: 0.910578\n",
      "Epoch 20/100: Loss: 0.4119, Val AUC: 0.911728\n",
      "Epoch 30/100: Loss: 0.4193, Val AUC: 0.909259\n",
      "Early stopping at epoch 31\n",
      "Epoch 10/100: Loss: 0.4265, Val AUC: 0.910382\n",
      "Epoch 20/100: Loss: 0.4299, Val AUC: 0.908698\n",
      "Epoch 30/100: Loss: 0.3996, Val AUC: 0.909512\n",
      "Early stopping at epoch 37\n",
      "Fold 4 Ensemble AUC: 0.907884\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Train size: 1752, Validation size: 438, Test size: 730\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4354\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's auc: 0.895286\n",
      "LightGBM validation AUC: 0.895286\n",
      "\n",
      "Top 10 important features for LightGBM:\n",
      "                    Feature   Importance\n",
      "7                     cloud  5258.358208\n",
      "20  weighted_cloud_humidity  4190.722396\n",
      "16   cloud_humidity_product  2913.783218\n",
      "33     cloud_sunshine_ratio  1415.684212\n",
      "17     cloud_humidity_ratio   602.391508\n",
      "35                   wind_v   427.855206\n",
      "19         cloud_normalized   381.691887\n",
      "5                  dewpoint   338.460202\n",
      "8                  sunshine   304.289732\n",
      "2                   maxtemp   243.824170\n",
      "\n",
      "Training XGBoost model...\n",
      "[0]\ttrain-auc:0.89638\tval-auc:0.86619\n",
      "[92]\ttrain-auc:0.94262\tval-auc:0.88883\n",
      "XGBoost validation AUC: 0.888833\n",
      "\n",
      "Training k-NN model...\n",
      "k-NN validation AUC: 0.860382\n",
      "Epoch 10/100: Loss: 0.4103, Val AUC: 0.891414\n",
      "Epoch 20/100: Loss: 0.4055, Val AUC: 0.895763\n",
      "Epoch 30/100: Loss: 0.4126, Val AUC: 0.890516\n",
      "Epoch 40/100: Loss: 0.3822, Val AUC: 0.890629\n",
      "Epoch 50/100: Loss: 0.3852, Val AUC: 0.888440\n",
      "Early stopping at epoch 56\n",
      "Epoch 10/100: Loss: 0.4227, Val AUC: 0.892200\n",
      "Epoch 20/100: Loss: 0.4335, Val AUC: 0.896184\n",
      "Epoch 30/100: Loss: 0.4049, Val AUC: 0.893715\n",
      "Early stopping at epoch 35\n",
      "Fold 5 Ensemble AUC: 0.886504\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "Neural Network OOF AUC: 0.885040\n",
      "LightGBM OOF AUC: 0.846819\n",
      "XGBoost OOF AUC: 0.861743\n",
      "k-NN OOF AUC: 0.859582\n",
      "Mean Fold Ensemble AUC: 0.886094  0.024478\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                    Feature   Importance\n",
      "5    cloud_humidity_product  9665.332012\n",
      "32  weighted_cloud_humidity  6801.817026\n",
      "1                     cloud  4223.188114\n",
      "8      cloud_sunshine_ratio  1966.974635\n",
      "7          cloud_normalized   764.597141\n",
      "34                   wind_v   701.828738\n",
      "6      cloud_humidity_ratio   686.294582\n",
      "26                 sunshine   575.410340\n",
      "11                 dewpoint   353.232569\n",
      "21                  maxtemp   329.349107\n",
      "0             calc_humidity   321.315903\n",
      "33                   wind_u   290.436555\n",
      "9                       day   239.423431\n",
      "28               temp_range   233.294670\n",
      "23                  mintemp   197.867752\n",
      "\n",
      "=== Creating Final Ensemble ===\n",
      "\n",
      "=== Optimized Ensemble Weights ===\n",
      "LightGBM: 0.2500 (25.0%)\n",
      "XGBoost: 0.2500 (25.0%)\n",
      "k-NN: 0.2500 (25.0%)\n",
      "Neural Network: 0.2500 (25.0%)\n",
      "Optimized ensemble AUC: 0.880827\n",
      "\n",
      "=== Creating Advanced Weather Pattern Adaptive Ensemble ===\n",
      "Cluster distribution:\n",
      "Cluster 0: 329 samples (15.0%)\n",
      "Cluster 1: 394 samples (18.0%)\n",
      "Cluster 2: 157 samples (7.2%)\n",
      "Cluster 3: 380 samples (17.4%)\n",
      "Cluster 4: 260 samples (11.9%)\n",
      "Cluster 5: 670 samples (30.6%)\n",
      "\n",
      "Optimizing for weather cluster 0\n",
      "  Model 1 AUC in cluster 0: 0.590643\n",
      "  Model 2 AUC in cluster 0: 0.652894\n",
      "  Model 3 AUC in cluster 0: 0.628851\n",
      "  Model 4 AUC in cluster 0: 0.693126\n",
      "  Cluster 0 weights: [0.21130085 0.25818812 0.2395229  0.29098813]\n",
      "\n",
      "Optimizing for weather cluster 1\n",
      "  Model 1 AUC in cluster 1: 0.565885\n",
      "  Model 2 AUC in cluster 1: 0.609505\n",
      "  Model 3 AUC in cluster 1: 0.648698\n",
      "  Model 4 AUC in cluster 1: 0.660677\n",
      "  Cluster 1 weights: [0.20672751 0.23982591 0.27166036 0.28178622]\n",
      "\n",
      "Optimizing for weather cluster 2\n",
      "  Model 1 AUC in cluster 2: 0.654242\n",
      "  Model 2 AUC in cluster 2: 0.697576\n",
      "  Model 3 AUC in cluster 2: 0.701364\n",
      "  Model 4 AUC in cluster 2: 0.636970\n",
      "  Cluster 2 weights: [0.23618405 0.26850719 0.27143113 0.22387763]\n",
      "\n",
      "Optimizing for weather cluster 3\n",
      "  Model 1 AUC in cluster 3: 0.672724\n",
      "  Model 2 AUC in cluster 3: 0.667457\n",
      "  Model 3 AUC in cluster 3: 0.625616\n",
      "  Model 4 AUC in cluster 3: 0.672814\n",
      "  Cluster 3 weights: [0.25977238 0.25572122 0.22466461 0.25984179]\n",
      "\n",
      "Optimizing for weather cluster 4\n",
      "  Model 1 AUC in cluster 4: 0.657765\n",
      "  Model 2 AUC in cluster 4: 0.660874\n",
      "  Model 3 AUC in cluster 4: 0.602679\n",
      "  Model 4 AUC in cluster 4: 0.673310\n",
      "  Cluster 4 weights: [0.2566195  0.25905116 0.21543689 0.26889246]\n",
      "\n",
      "Optimizing for weather cluster 5\n",
      "  Model 1 AUC in cluster 5: 0.512253\n",
      "  Model 2 AUC in cluster 5: 0.550196\n",
      "  Model 3 AUC in cluster 5: 0.561984\n",
      "  Model 4 AUC in cluster 5: 0.666137\n",
      "  Cluster 5 weights: [0.19808775 0.22851919 0.23841616 0.3349769 ]\n",
      "\n",
      "Calibrating final predictions...\n",
      "Original predictions - Mean: 0.7553, Std: 0.2190\n",
      "Calibrated predictions - Mean: 0.8157, Std: 0.3870\n",
      "\n",
      "[4/5] Creating detailed visualizations for analysis...\n",
      "Creating detailed visualizations...\n",
      "Creating feature effect visualizations...\n",
      "\n",
      "Visualizations have been saved to: C:/Nauka/Koo naukowe/Binary Prediction with a Rainfall Dataset\\visualizations\n",
      "Visualization files:\n",
      " - feature_importance.png\n",
      " - prediction_distribution.png\n",
      " - weather_pattern_analysis.png\n",
      " - model_comparison.png\n",
      " - feature_effects.png\n",
      "\n",
      "[5/5] Creating submission file...\n",
      "Submission file saved to: C:/Nauka/Koo naukowe/Binary Prediction with a Rainfall Dataset\\rainfall_predictions.csv\n",
      "[LightGBM] [Info] Number of positive: 1320, number of negative: 432\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4332\n",
      "[LightGBM] [Info] Number of data points in the train set: 1752, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.753425 -> initscore=1.116961\n",
      "[LightGBM] [Info] Start training from score 1.116961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Could not perform error analysis: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "\n",
      "=== Submission Summary ===\n",
      "First 5 rows of submission file:\n",
      "     id  rainfall\n",
      "0  2190  0.965053\n",
      "1  2191  0.966633\n",
      "2  2192  0.940104\n",
      "3  2193  0.180899\n",
      "4  2194  0.161305\n",
      "\n",
      "Prediction statistics:\n",
      "Min: 0.1263, Max: 0.9669\n",
      "Mean: 0.7734, Std: 0.2608\n",
      "Median: 0.9071\n",
      "\n",
      "Run time: 1.07 minutes\n",
      "\n",
      "=== Model Training Complete ===\n",
      "This optimized ensemble model incorporates:\n",
      "- Meteorological feature engineering based on physical rainfall processes\n",
      "- Weather pattern clustering with adaptive ensemble techniques\n",
      "- Multiple complementary models (LightGBM, XGBoost, k-NN, Neural Network)\n",
      "- Comprehensive visualization and model interpretation\n",
      "\n",
      "All results and visualizations saved to: C:/Nauka/Koo naukowe/Binary Prediction with a Rainfall Dataset\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced Rainfall Prediction Model\n",
    "Based on insights from top competition solutions with enhanced visualization\n",
    "Adapted for local file paths\n",
    "\"\"\"\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Try importing optional libraries\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    has_lightgbm = True\n",
    "except ImportError:\n",
    "    has_lightgbm = False\n",
    "    print(\"LightGBM not available. Install with: pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    has_xgboost = True\n",
    "except ImportError:\n",
    "    has_xgboost = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "    has_torch = True\n",
    "except ImportError:\n",
    "    has_torch = False\n",
    "    print(\"PyTorch not available. Install with: pip install torch\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    has_catboost = True\n",
    "except ImportError:\n",
    "    has_catboost = False\n",
    "    print(\"CatBoost not available. Consider installing with: pip install catboost\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "if has_torch:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Set device for PyTorch if available\n",
    "if has_torch:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"Using device: {device} (PyTorch not available)\")\n",
    "\n",
    "# Initialize global variable for temperature column name\n",
    "temp_col = 'temperature'  # will be updated during preprocessing\n",
    "\n",
    "def load_data(base_path=\"C:/Nauka/Koo naukowe/Binary Prediction with a Rainfall Dataset\"):\n",
    "    \"\"\"\n",
    "    Load data files from the local directory path\n",
    "    \n",
    "    Returns:\n",
    "        train: Training data\n",
    "        test: Test data\n",
    "        submission: Sample submission\n",
    "        base_path: Base path for data (to be used for saving results)\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
    "    \n",
    "    print(f\"Training data shape: {train.shape}\")\n",
    "    print(f\"Test data shape: {test.shape}\")\n",
    "    \n",
    "    # Create empty sample_submission if needed\n",
    "    sample_submission = pd.DataFrame({'id': test['id'], 'rainfall': 0})\n",
    "    \n",
    "    return train, test, sample_submission, base_path\n",
    "\n",
    "def calculate_dewpoint_spread(temp, dewpoint):\n",
    "    \"\"\"Calculate dewpoint spread, a key meteorological variable for precipitation\"\"\"\n",
    "    return temp - dewpoint\n",
    "\n",
    "def calculate_relative_humidity(temp, dewpoint):\n",
    "    \"\"\"\n",
    "    Calculate relative humidity from temperature and dewpoint\n",
    "    Uses the August-Roche-Magnus approximation\n",
    "    \"\"\"\n",
    "    # Constants for the formula\n",
    "    a = 17.625\n",
    "    b = 243.04  # C\n",
    "    \n",
    "    # Calculate saturation vapor pressure\n",
    "    rh = 100 * np.exp(a * (dewpoint / (b + dewpoint)) - a * (temp / (b + temp)))\n",
    "    return np.clip(rh, 0, 100)  # Clip to valid range\n",
    "\n",
    "def calculate_vapor_pressure_deficit(temp, dewpoint):\n",
    "    \"\"\"\n",
    "    Calculate Vapor Pressure Deficit (VPD) - an important metric for evaporation potential\n",
    "    Higher VPD means more evaporation and less rainfall potential\n",
    "    \"\"\"\n",
    "    # Constants for the formula\n",
    "    a = 0.611  # kPa\n",
    "    b = 17.502\n",
    "    c = 240.97  # C\n",
    "    \n",
    "    # Calculate saturation vapor pressure\n",
    "    es = a * np.exp((b * temp) / (c + temp))\n",
    "    \n",
    "    # Calculate actual vapor pressure\n",
    "    ea = a * np.exp((b * dewpoint) / (c + dewpoint))\n",
    "    \n",
    "    # VPD is the difference\n",
    "    vpd = es - ea\n",
    "    return np.clip(vpd, 0, None)  # Cannot be negative\n",
    "\n",
    "def create_optimized_weather_features(df):\n",
    "    \"\"\"\n",
    "    Create enhanced meteorological features with better predictive power\n",
    "    Based on insights from top solutions - focused on high-impact features\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # ===== TEMPERATURE AND MOISTURE FEATURES =====\n",
    "    # Temperature features\n",
    "    if 'maxtemp' in df_new.columns and 'mintemp' in df_new.columns:\n",
    "        df_new['temp_range'] = df_new['maxtemp'] - df_new['mintemp']\n",
    "        \n",
    "    # Fix spelling of temperature and calculate dewpoint features\n",
    "    global temp_col\n",
    "    temp_col = 'temperature' if 'temperature' in df_new.columns else 'temparature'\n",
    "    \n",
    "    df_new['dew_depression'] = calculate_dewpoint_spread(df_new[temp_col], df_new['dewpoint'])\n",
    "    df_new['calc_humidity'] = calculate_relative_humidity(df_new[temp_col], df_new['dewpoint'])\n",
    "    df_new['humidity_deficit'] = 100 - df_new['humidity']\n",
    "    df_new['vapor_pressure_deficit'] = calculate_vapor_pressure_deficit(df_new[temp_col], df_new['dewpoint'])\n",
    "    \n",
    "    # ===== CLOUD AND PRECIPITATION INDICATORS =====\n",
    "    # Primary rainfall predictors (report indicated these were crucial)\n",
    "    df_new['cloud_humidity_product'] = df_new['cloud'] * df_new['humidity']\n",
    "    df_new['cloud_humidity_ratio'] = df_new['cloud'] / df_new['humidity'].replace(0, 0.1)\n",
    "    \n",
    "    # Normalized values\n",
    "    df_new['humidity_normalized'] = df_new['humidity'] / 100\n",
    "    df_new['cloud_normalized'] = df_new['cloud'] / 10\n",
    "    \n",
    "    # Critical cloud-humidity interaction (identified as one of the most important features)\n",
    "    df_new['weighted_cloud_humidity'] = (df_new['cloud'] ** 1.5) * (df_new['humidity'] ** 0.8) / 100\n",
    "    \n",
    "    # Threshold features (highly effective according to report)\n",
    "    for threshold in [70, 80, 90]:\n",
    "        df_new[f'humidity_above_{threshold}'] = (df_new['humidity'] > threshold).astype(int)\n",
    "    \n",
    "    for threshold in [6, 7, 8]:\n",
    "        df_new[f'cloud_above_{threshold}'] = (df_new['cloud'] > threshold).astype(int)\n",
    "    \n",
    "    # Combined thresholds (strong rain predictors)\n",
    "    df_new['high_rain_potential'] = ((df_new['cloud'] > 7) & (df_new['humidity'] > 85)).astype(int)\n",
    "    df_new['medium_rain_potential'] = ((df_new['cloud'] > 5) & (df_new['cloud'] <= 7) & \n",
    "                                      (df_new['humidity'] > 75)).astype(int)\n",
    "    \n",
    "    # ===== PRESSURE FEATURES =====\n",
    "    # Pressure anomaly (deviation from standard atmosphere)\n",
    "    df_new['pressure_delta'] = df_new['pressure'] - 1013.25\n",
    "    \n",
    "    # Pressure categories (low pressure systems often bring rain)\n",
    "    df_new['low_pressure'] = (df_new['pressure'] < 1000).astype(int)\n",
    "    df_new['high_pressure'] = (df_new['pressure'] > 1020).astype(int)\n",
    "    \n",
    "    # ===== SUNSHINE AND CLOUD INTERACTIONS =====\n",
    "    # Sunshine features\n",
    "    df_new['sunshine_deficit'] = 10 - df_new['sunshine']\n",
    "    df_new['cloud_sunshine_ratio'] = df_new['cloud'] / (df_new['sunshine'] + 0.1)\n",
    "    \n",
    "    # ===== WIND VECTOR COMPONENTS =====\n",
    "    # Convert wind direction to cartesian components (better for modeling)\n",
    "    wind_rad = np.radians(df_new['winddirection'])\n",
    "    df_new['wind_u'] = df_new['windspeed'] * np.sin(wind_rad)  # East component\n",
    "    df_new['wind_v'] = df_new['windspeed'] * np.cos(wind_rad)  # North component\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def perform_kmeans_clustering(features, n_clusters=6):\n",
    "    \"\"\"Create cluster assignments for weather patterns\"\"\"\n",
    "    # Select key meteorological features for clustering\n",
    "    global temp_col\n",
    "    cluster_features = ['cloud', 'humidity', temp_col, 'pressure', 'windspeed', 'sunshine',\n",
    "                       'cloud_humidity_product']  # More focused feature set based on report\n",
    "    \n",
    "    # Prepare data for clustering\n",
    "    cluster_data = features[cluster_features].copy()\n",
    "    \n",
    "    # Fill any missing values\n",
    "    cluster_data = cluster_data.fillna(cluster_data.mean())\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "    clusters = kmeans.fit_predict(cluster_data_scaled)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def select_optimal_features(features, target):\n",
    "    \"\"\"\n",
    "    Select the most important features based on model importance\n",
    "    The report indicated that feature selection was important\n",
    "    \"\"\"\n",
    "    if not has_lightgbm:\n",
    "        print(\"LightGBM not available for feature selection. Using all features.\")\n",
    "        return features.columns.tolist()\n",
    "    \n",
    "    # Set up LightGBM for feature selection\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'seed': SEED\n",
    "    }\n",
    "    \n",
    "    # Create dataset\n",
    "    lgb_data = lgb.Dataset(features, label=target)\n",
    "    \n",
    "    # Train a simple model\n",
    "    model = lgb.train(lgb_params, lgb_data, num_boost_round=100)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': features.columns,\n",
    "        'Importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Get the most important features (adjust threshold based on your needs)\n",
    "    top_features = importance.head(40)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"Selected {len(top_features)} optimal features out of {features.shape[1]}\")\n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "def preprocess_data(train, test, target_col='rainfall', id_col='id'):\n",
    "    \"\"\"Enhanced preprocessing with improved scaling and feature selection\"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Extract IDs and target\n",
    "    train_ids = train[id_col].copy() if id_col in train.columns else None\n",
    "    test_ids = test[id_col].copy() if id_col in test.columns else None\n",
    "    \n",
    "    if target_col in train.columns:\n",
    "        target = train[target_col].copy()\n",
    "        print(f\"Target distribution: {target.value_counts(normalize=True) * 100}\")\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    features = train.drop([id_col, target_col], axis=1, errors='ignore')\n",
    "    test_features = test.drop([id_col], axis=1, errors='ignore')\n",
    "    \n",
    "    # Fix temperature column name if needed\n",
    "    global temp_col\n",
    "    temp_col = 'temperature' if 'temperature' in features.columns else 'temparature'\n",
    "    \n",
    "    # Add enhanced meteorological features\n",
    "    print(\"Creating optimized meteorological features...\")\n",
    "    features = create_optimized_weather_features(features)\n",
    "    test_features = create_optimized_weather_features(test_features)\n",
    "    \n",
    "    # Perform clustering for weather patterns\n",
    "    print(\"Creating weather pattern clusters...\")\n",
    "    features['weather_cluster'] = perform_kmeans_clustering(features)\n",
    "    test_features['weather_cluster'] = perform_kmeans_clustering(test_features)\n",
    "    \n",
    "    # Create improved validation folds (crucial according to report)\n",
    "    print(\"Creating stratified folds for cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    features['fold'] = -1\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(features, target)):\n",
    "        features.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "    # Optional: Feature selection (uncomment if you want to reduce feature set)\n",
    "    # top_features = select_optimal_features(features.drop('fold', axis=1), target)\n",
    "    # features = features[top_features + ['fold', 'weather_cluster']]\n",
    "    # test_features = test_features[top_features + ['weather_cluster']]\n",
    "    \n",
    "    # Different scaling strategies for different feature types\n",
    "    print(\"Applying feature scaling...\")\n",
    "    \n",
    "    # Identify column types for specialized scaling\n",
    "    standard_features = [temp_col, 'dewpoint', 'pressure', 'wind_u', 'wind_v']\n",
    "    \n",
    "    quantile_features = ['humidity', 'cloud', 'windspeed', 'cloud_humidity_product']\n",
    "    \n",
    "    minmax_features = ['temp_range', 'dew_depression', 'sunshine', 'sunshine_deficit', \n",
    "                       'vapor_pressure_deficit', 'cloud_sunshine_ratio']\n",
    "    \n",
    "    # Apply different scaling to each group (outside of fold splits to prevent data leakage)\n",
    "    std_scaler = StandardScaler()\n",
    "    qt_scaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    \n",
    "    for col in standard_features:\n",
    "        if col in features.columns:\n",
    "            features[col] = std_scaler.fit_transform(features[[col]])\n",
    "            test_features[col] = std_scaler.transform(test_features[[col]])\n",
    "            \n",
    "    for col in quantile_features:\n",
    "        if col in features.columns:\n",
    "            features[col] = qt_scaler.fit_transform(features[[col]])\n",
    "            test_features[col] = qt_scaler.transform(test_features[[col]])\n",
    "            \n",
    "    for col in minmax_features:\n",
    "        if col in features.columns:\n",
    "            features[col] = mm_scaler.fit_transform(features[[col]])\n",
    "            test_features[col] = mm_scaler.transform(test_features[[col]])\n",
    "    \n",
    "    # Handle any remaining inf and NaN values\n",
    "    features = features.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    test_features = test_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    print(f\"Preprocessing complete. Features: {features.shape[1]}\")\n",
    "    return features, test_features, target, train_ids, test_ids\n",
    "\n",
    "class RainfallDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for rainfall prediction\"\"\"\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        \n",
    "        if targets is not None:\n",
    "            self.targets = torch.tensor(targets.values, dtype=torch.float32).reshape(-1, 1)\n",
    "        else:\n",
    "            self.targets = None\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.features[idx], self.targets[idx]\n",
    "        else:\n",
    "            return self.features[idx]\n",
    "\n",
    "class RainfallPredictionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP network optimized for rainfall prediction\n",
    "    Based on insights from successful solutions - simple but effective\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple but effective architecture based on report insights\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation (better than ReLU according to report)\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Remove the erroneous closing square bracket here\n",
    "\n",
    "def train_pytorch_model(train_data, val_data, test_data, params):\n",
    "    \"\"\"Train a PyTorch neural network model with optimized parameters\"\"\"\n",
    "    if not has_torch:\n",
    "        print(\"PyTorch not available. Skipping neural network model.\")\n",
    "        return np.zeros(len(test_data)), 0.5  # Return dummy predictions\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = RainfallDataset(X_train, y_train)\n",
    "    val_dataset = RainfallDataset(X_val, y_val)\n",
    "    test_dataset = RainfallDataset(test_data)\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    class_counts = y_train.value_counts().to_dict()\n",
    "    total = len(y_train)\n",
    "    \n",
    "    if 0 in class_counts and 1 in class_counts:\n",
    "        # Boost weight for minority class (no rain)\n",
    "        neg_weight = total / (2.0 * class_counts[0]) * 1.2  # Extra weight for minority class\n",
    "        pos_weight = total / (2.0 * class_counts[1])\n",
    "        samples_per_class = {0: neg_weight, 1: pos_weight}\n",
    "    else:\n",
    "        samples_per_class = {c: total / float(count) for c, count in class_counts.items()}\n",
    "    \n",
    "    weights = y_train.map(samples_per_class).values\n",
    "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = params.get('batch_size', 64)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dim = params.get('hidden_dim', 128)\n",
    "    dropout_rate = params.get('dropout_rate', 0.25)\n",
    "    \n",
    "    model = RainfallPredictionNetwork(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = params.get('learning_rate', 0.001)\n",
    "    weight_decay = params.get('weight_decay', 1e-4)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=10, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping setup\n",
    "    patience = params.get('patience', 20)\n",
    "    best_auc = 0\n",
    "    best_model = None\n",
    "    no_improve = 0\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = params.get('epochs', 100)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features = features.to(device)\n",
    "                outputs = model(features)\n",
    "                val_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(targets.cpu().numpy().flatten())\n",
    "        \n",
    "        # Calculate validation AUC\n",
    "        val_auc = roc_auc_score(val_targets, val_preds)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Loss: {train_loss/len(train_loader):.4f}, Val AUC: {val_auc:.6f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_model = model.state_dict().copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    # Generate predictions\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features_batch in test_loader:\n",
    "            if isinstance(features_batch, list) or isinstance(features_batch, tuple):\n",
    "                features_batch = features_batch[0]  # Only get features if loader returns a tuple\n",
    "            features_batch = features_batch.to(device)\n",
    "            outputs = model(features_batch)\n",
    "            test_preds.extend(outputs.cpu().numpy().flatten())\n",
    "    \n",
    "    return np.array(test_preds), best_auc\n",
    "\n",
    "def get_lgb_params():\n",
    "    \"\"\"Optimized parameters for LightGBM based on report insights\"\"\"\n",
    "    return {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,  # Lower from 41 to reduce overfitting\n",
    "        'max_depth': 6,    # Lower from 8 for better generalization\n",
    "        'min_data_in_leaf': 30,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'lambda_l1': 0.1,  # L1 regularization\n",
    "        'lambda_l2': 1.5,  # Increased L2 regularization\n",
    "        'min_gain_to_split': 0.02,\n",
    "        'seed': SEED,\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "\n",
    "def get_xgb_params():\n",
    "    \"\"\"Optimized parameters for XGBoost\"\"\"\n",
    "    return {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 3,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'gamma': 0.1,\n",
    "        'alpha': 0.3,\n",
    "        'lambda': 1.5,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': SEED,\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "\n",
    "def train_lightgbm(X_train, y_train, X_val, y_val, X_test):\n",
    "    \"\"\"Train an optimized LightGBM model\"\"\"\n",
    "    if not has_lightgbm:\n",
    "        print(\"LightGBM not available. Skipping LightGBM model.\")\n",
    "        return np.zeros(len(X_test)), np.zeros(len(X_val)), 0.5, None  # Return dummy predictions\n",
    "    \n",
    "    print(\"\\nTraining LightGBM model...\")\n",
    "    \n",
    "    # Get parameters\n",
    "    params = get_lgb_params()\n",
    "    \n",
    "    # Set class weights to handle imbalance\n",
    "    # The report mentions imbalance of ~75% rain, 25% no rain\n",
    "    class_counts = y_train.value_counts()\n",
    "    total = len(y_train)\n",
    "    \n",
    "    # If we have both classes, calculate weights\n",
    "    if len(class_counts) > 1:\n",
    "        # Boost weight for minority class (typically 0 - no rain)\n",
    "        params['scale_pos_weight'] = class_counts[1] / class_counts[0]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,  # Will be early-stopped\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=params['early_stopping_rounds']),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_preds = model.predict(X_val)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate validation AUC\n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"LightGBM validation AUC: {val_auc:.6f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display top 10 important features\n",
    "    print(\"\\nTop 10 important features for LightGBM:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    return test_preds, val_preds, val_auc, importance\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test):\n",
    "    \"\"\"Train an optimized XGBoost model\"\"\"\n",
    "    if not has_xgboost:\n",
    "        print(\"XGBoost not available. Skipping XGBoost model.\")\n",
    "        return np.zeros(len(X_test)), np.zeros(len(X_val)), 0.5  # Return dummy predictions\n",
    "    \n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    \n",
    "    # Get parameters\n",
    "    params = get_xgb_params()\n",
    "    \n",
    "    # Set class weights for imbalance\n",
    "    class_counts = y_train.value_counts()\n",
    "    if len(class_counts) > 1:\n",
    "        params['scale_pos_weight'] = class_counts[1] / class_counts[0]\n",
    "    \n",
    "    # Create DMatrix objects\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=params['early_stopping_rounds'],\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_preds = model.predict(dval)\n",
    "    test_preds = model.predict(dtest)\n",
    "    \n",
    "    # Calculate validation AUC\n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"XGBoost validation AUC: {val_auc:.6f}\")\n",
    "    \n",
    "    return test_preds, val_preds, val_auc\n",
    "\n",
    "def train_knn(X_train, y_train, X_val, y_val, X_test):\n",
    "    \"\"\"\n",
    "    Train a k-NN model\n",
    "    The report highlighted that k-NN was valuable in the ensemble\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining k-NN model...\")\n",
    "    \n",
    "    # Use k-NN with optimized parameters\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=15,\n",
    "        weights='distance',\n",
    "        metric='minkowski',\n",
    "        p=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_preds = model.predict_proba(X_val)[:, 1]\n",
    "    test_preds = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate validation AUC\n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"k-NN validation AUC: {val_auc:.6f}\")\n",
    "    \n",
    "    return test_preds, val_preds, val_auc\n",
    "\n",
    "def simple_weighted_ensemble(predictions, weights=None):\n",
    "    \"\"\"\n",
    "    Create a weighted ensemble from multiple model predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction arrays\n",
    "        weights: List of weights (will be normalized) or None for equal weighting\n",
    "        \n",
    "    Returns:\n",
    "        Weighted average of predictions\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1] * len(predictions)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights) / sum(weights)\n",
    "    \n",
    "    # Weighted average\n",
    "    ensemble = np.zeros_like(predictions[0])\n",
    "    for pred, weight in zip(predictions, weights):\n",
    "        ensemble += weight * pred\n",
    "        \n",
    "    return ensemble\n",
    "\n",
    "def optimize_ensemble_weights(oof_predictions, target):\n",
    "    \"\"\"\n",
    "    Optimize the weights for ensemble using scipy optimization\n",
    "    This is a technique used by Kaggle Grandmasters to get the last bit of performance\n",
    "    \"\"\"\n",
    "    # Define the objective function to minimize (negative AUC)\n",
    "    def objective(weights):\n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Calculate weighted ensemble\n",
    "        weighted_preds = np.zeros_like(oof_predictions[0])\n",
    "        for i, pred in enumerate(oof_predictions):\n",
    "            weighted_preds += weights[i] * pred\n",
    "            \n",
    "        # Return negative AUC (since we want to maximize AUC)\n",
    "        return -roc_auc_score(target, weighted_preds)\n",
    "    \n",
    "    # Initial weights (equal)\n",
    "    initial_weights = np.ones(len(oof_predictions)) / len(oof_predictions)\n",
    "    \n",
    "    # Add constraint: weights must be positive\n",
    "    constraints = ({'type': 'ineq', 'fun': lambda w: w})\n",
    "    \n",
    "    # Run optimization\n",
    "    result = minimize(objective, initial_weights, constraints=constraints, method='SLSQP')\n",
    "    \n",
    "    # Get optimal weights\n",
    "    optimal_weights = result['x'] / np.sum(result['x'])\n",
    "    \n",
    "    # Calculate the ensemble AUC with these weights\n",
    "    ensemble_preds = np.zeros_like(oof_predictions[0])\n",
    "    for i, pred in enumerate(oof_predictions):\n",
    "        ensemble_preds += optimal_weights[i] * pred\n",
    "    ensemble_auc = roc_auc_score(target, ensemble_preds)\n",
    "    \n",
    "    print(\"\\n=== Optimized Ensemble Weights ===\")\n",
    "    model_names = ['LightGBM', 'XGBoost', 'k-NN', 'Neural Network']\n",
    "    for name, weight in zip(model_names, optimal_weights):\n",
    "        print(f\"{name}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "    print(f\"Optimized ensemble AUC: {ensemble_auc:.6f}\")\n",
    "    \n",
    "    return optimal_weights, ensemble_auc\n",
    "\n",
    "def adaptive_weather_ensemble(predictions, oof_predictions, target, features, test_features, base_path):\n",
    "    \"\"\"\n",
    "    Enhanced weather-adaptive ensemble with cluster optimization and outlier handling\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of test predictions from base models\n",
    "        oof_predictions: List of out-of-fold predictions\n",
    "        target: Target variable\n",
    "        features: Training feature DataFrame with weather_cluster\n",
    "        test_features: Test feature DataFrame with weather_cluster\n",
    "        base_path: Base path for saving visualizations\n",
    "        \n",
    "    Returns:\n",
    "        Weather-adaptive ensemble predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Creating Advanced Weather Pattern Adaptive Ensemble ===\")\n",
    "    \n",
    "    # Create directory for visualizations\n",
    "    viz_dir = os.path.join(base_path, 'visualizations')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    if 'weather_cluster' not in features.columns or 'weather_cluster' not in test_features.columns:\n",
    "        print(\"Weather cluster information not available. Using standard ensemble.\")\n",
    "        return simple_weighted_ensemble(predictions)\n",
    "    \n",
    "    # Convert predictions to DataFrames for easier handling\n",
    "    oof_df = pd.DataFrame(np.column_stack(oof_predictions), \n",
    "                         columns=[f'model_{i}' for i in range(len(oof_predictions))])\n",
    "    oof_df['weather_cluster'] = features['weather_cluster'].values\n",
    "    oof_df['target'] = target.values\n",
    "    \n",
    "    test_df = pd.DataFrame(np.column_stack(predictions),\n",
    "                          columns=[f'model_{i}' for i in range(len(predictions))])\n",
    "    test_df['weather_cluster'] = test_features['weather_cluster'].values\n",
    "    \n",
    "    # Initialize final predictions\n",
    "    final_preds = np.zeros(len(test_df))\n",
    "    \n",
    "    # Get cluster distribution stats\n",
    "    cluster_counts = oof_df['weather_cluster'].value_counts().sort_index()\n",
    "    print(\"Cluster distribution:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} samples ({count/len(oof_df)*100:.1f}%)\")\n",
    "    \n",
    "    # For each cluster, calculate optimal weights\n",
    "    n_clusters = features['weather_cluster'].nunique()\n",
    "    all_weights = []\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nOptimizing for weather cluster {cluster}\")\n",
    "        \n",
    "        # Get cluster data\n",
    "        cluster_oof = oof_df[oof_df['weather_cluster'] == cluster]\n",
    "        cluster_test = test_df[test_df['weather_cluster'] == cluster]\n",
    "        \n",
    "        if len(cluster_oof) < 20:  # Increased minimum size threshold\n",
    "            print(f\"Too few samples ({len(cluster_oof)}) for cluster {cluster}. Using overall weights.\")\n",
    "            \n",
    "            # Use robust cross-validated weights for small clusters\n",
    "            from sklearn.model_selection import KFold\n",
    "            overall_aucs = []\n",
    "            \n",
    "            # Estimate AUCs more robustly with cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            for train_idx, val_idx in kf.split(oof_df):\n",
    "                model_aucs = []\n",
    "                for i in range(len(oof_predictions)):\n",
    "                    col = f'model_{i}'\n",
    "                    # Calculate AUC on validation fold\n",
    "                    auc = roc_auc_score(oof_df.iloc[val_idx]['target'], \n",
    "                                       oof_df.iloc[val_idx][col])\n",
    "                    model_aucs.append(auc)\n",
    "                overall_aucs.append(model_aucs)\n",
    "            \n",
    "            # Average AUCs across folds\n",
    "            weights = np.mean(overall_aucs, axis=0)\n",
    "            \n",
    "        else:\n",
    "            # Calculate AUC for each model in this cluster with outlier handling\n",
    "            cluster_aucs = []\n",
    "            for i in range(len(oof_predictions)):\n",
    "                col = f'model_{i}'\n",
    "                \n",
    "                # Check for potential outliers (predictions far from others)\n",
    "                preds = cluster_oof[col].values\n",
    "                \n",
    "                # Handle any potential issues with extreme predictions\n",
    "                preds_clean = np.clip(preds, 0.001, 0.999)  # Avoid extreme values\n",
    "                \n",
    "                # Calculate AUC\n",
    "                auc = roc_auc_score(cluster_oof['target'], preds_clean)\n",
    "                cluster_aucs.append(auc)\n",
    "                print(f\"  Model {i+1} AUC in cluster {cluster}: {auc:.6f}\")\n",
    "            \n",
    "            # Use model AUCs as weights\n",
    "            weights = cluster_aucs\n",
    "        \n",
    "        # Normalize weights and square them to emphasize better models\n",
    "        weights = np.array([w**2 for w in weights])\n",
    "        weights = weights / sum(weights)\n",
    "        print(f\"  Cluster {cluster} weights: {weights}\")\n",
    "        all_weights.append(weights)\n",
    "        \n",
    "        # Create weighted predictions for this cluster\n",
    "        cluster_indices = cluster_test.index\n",
    "        for i, col in enumerate(f'model_{i}' for i in range(len(predictions))):\n",
    "            final_preds[cluster_indices] += weights[i] * cluster_test[col].values\n",
    "    \n",
    "    # Create a visualization of model weights by cluster\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    all_weights = np.array(all_weights)\n",
    "    model_names = ['LightGBM', 'XGBoost', 'k-NN', 'Neural Network']\n",
    "    \n",
    "    # Plot stacked bars showing weight distribution\n",
    "    bottom = np.zeros(n_clusters)\n",
    "    for i, model in enumerate(model_names):\n",
    "        plt.bar(range(n_clusters), all_weights[:, i], bottom=bottom, label=model)\n",
    "        bottom += all_weights[:, i]\n",
    "    \n",
    "    plt.title('Model Weight Distribution by Weather Cluster', fontsize=16)\n",
    "    plt.xlabel('Weather Cluster', fontsize=14)\n",
    "    plt.ylabel('Weight Proportion', fontsize=14)\n",
    "    plt.xticks(range(n_clusters))\n",
    "    plt.legend(title='Models')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    viz_path = os.path.join(viz_dir, 'cluster_weights.png')\n",
    "    plt.savefig(viz_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return final_preds\n",
    "\n",
    "def calibrate_predictions(predictions, method='isotonic', base_path=None):\n",
    "    \"\"\"\n",
    "    Calibrate prediction probabilities using either Platt Scaling or Isotonic Regression\n",
    "    This can improve probability estimates without changing rankings (AUC)\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions to calibrate\n",
    "        method: Calibration method ('isotonic' or 'platt')\n",
    "        base_path: Base path for saving visualizations\n",
    "    \"\"\"\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    print(\"\\nCalibrating final predictions...\")\n",
    "    \n",
    "    # Create visualization directory\n",
    "    if base_path:\n",
    "        viz_dir = os.path.join(base_path, 'visualizations')\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "    else:\n",
    "        viz_dir = 'visualizations'\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Prepare array for calibrated predictions\n",
    "    calibrated_preds = np.zeros_like(predictions)\n",
    "    \n",
    "    # Artificial target creation (for calibration purposes only)\n",
    "    # This simulates expected class distribution based on predictions\n",
    "    artificial_target = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Fit calibration models\n",
    "    for train_idx, cal_idx in kf.split(predictions):\n",
    "        # Train calibration model\n",
    "        if method == 'isotonic':\n",
    "            calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        else:  # Platt scaling\n",
    "            calibrator = LogisticRegression(C=1.0, solver='lbfgs')\n",
    "            \n",
    "        # Reshape for calibrator\n",
    "        X_train = predictions[train_idx].reshape(-1, 1)\n",
    "        y_train = artificial_target[train_idx]\n",
    "        \n",
    "        # Fit and predict\n",
    "        calibrator.fit(X_train, y_train)\n",
    "        \n",
    "        # Apply calibration\n",
    "        X_cal = predictions[cal_idx].reshape(-1, 1)\n",
    "        if method == 'isotonic':\n",
    "            calibrated_preds[cal_idx] = calibrator.predict(X_cal)\n",
    "        else:\n",
    "            calibrated_preds[cal_idx] = calibrator.predict_proba(X_cal)[:, 1]\n",
    "    \n",
    "    # Visualize calibration effect\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot original vs calibrated distributions\n",
    "    sns.kdeplot(predictions, label='Original Predictions', color='blue')\n",
    "    sns.kdeplot(calibrated_preds, label='Calibrated Predictions', color='red')\n",
    "    \n",
    "    plt.title(f'Effect of {method.capitalize()} Calibration on Predictions', fontsize=14)\n",
    "    plt.xlabel('Prediction Probability', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save visualization\n",
    "    viz_path = os.path.join(viz_dir, 'calibration_effect.png')\n",
    "    plt.savefig(viz_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Report calibration stats\n",
    "    print(f\"Original predictions - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n",
    "    print(f\"Calibrated predictions - Mean: {calibrated_preds.mean():.4f}, Std: {calibrated_preds.std():.4f}\")\n",
    "    \n",
    "    return calibrated_preds\n",
    "\n",
    "def run_cross_validation(features, target, test_features, base_path, n_folds=5):\n",
    "    \"\"\"\n",
    "    Improved cross-validation with visualization and ensemble optimization\n",
    "    Model saving functionality has been removed as it was unused.\n",
    "    \n",
    "    Args:\n",
    "        features: Training features DataFrame\n",
    "        target: Target Series\n",
    "        test_features: Test features DataFrame\n",
    "        base_path: Base path for saving visualizations\n",
    "        n_folds: Number of folds\n",
    "        \n",
    "    Returns:\n",
    "        Final predictions for test set and feature importance\n",
    "    \"\"\"\n",
    "    # Initialize arrays for OOF and test predictions\n",
    "    nn_oof = np.zeros(len(features))\n",
    "    lgb_oof = np.zeros(len(features))\n",
    "    xgb_oof = np.zeros(len(features))\n",
    "    knn_oof = np.zeros(len(features))\n",
    "    \n",
    "    nn_preds = np.zeros(len(test_features))\n",
    "    lgb_preds = np.zeros(len(test_features))\n",
    "    xgb_preds = np.zeros(len(test_features))\n",
    "    knn_preds = np.zeros(len(test_features))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # Get neural network hyperparameters\n",
    "    nn_params = {\n",
    "        'batch_size': 64,\n",
    "        'hidden_dim': 128,\n",
    "        'dropout_rate': 0.25,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-4,\n",
    "        'epochs': 100,\n",
    "        'patience': 20\n",
    "    }\n",
    "    \n",
    "    # Iterate through folds\n",
    "    fold_aucs = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold + 1}/{n_folds} ===\")\n",
    "        \n",
    "        # Get train and validation indices\n",
    "        train_mask = features['fold'] != fold\n",
    "        val_mask = features['fold'] == fold\n",
    "        \n",
    "        # Create train and validation sets\n",
    "        X_train = features.loc[train_mask].drop('fold', axis=1)\n",
    "        y_train = target.loc[train_mask]\n",
    "        X_val = features.loc[val_mask].drop('fold', axis=1)\n",
    "        y_val = target.loc[val_mask]\n",
    "        \n",
    "        # Drop fold column from test for inference\n",
    "        X_test = test_features.copy()\n",
    "        if 'fold' in X_test.columns:\n",
    "            X_test = X_test.drop('fold', axis=1)\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "        \n",
    "        # Train LightGBM\n",
    "        lgb_test_fold, lgb_val_fold, lgb_auc, lgb_importance = train_lightgbm(X_train, y_train, X_val, y_val, X_test)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        xgb_test_fold, xgb_val_fold, xgb_auc = train_xgboost(X_train, y_train, X_val, y_val, X_test)\n",
    "        \n",
    "        # Train k-NN (highlighted in the report as valuable)\n",
    "        knn_test_fold, knn_val_fold, knn_auc = train_knn(X_train, y_train, X_val, y_val, X_test)\n",
    "        \n",
    "        # Train neural network\n",
    "        nn_train_data = (X_train, y_train)\n",
    "        nn_val_data = (X_val, y_val)\n",
    "        nn_test_fold, nn_auc = train_pytorch_model(nn_train_data, nn_val_data, X_test, nn_params)\n",
    "        \n",
    "        # Generate validation predictions for neural network\n",
    "        nn_val_fold, _ = train_pytorch_model(nn_train_data, nn_val_data, X_val, nn_params)\n",
    "        \n",
    "        # Store OOF predictions\n",
    "        nn_oof[val_mask] = nn_val_fold\n",
    "        lgb_oof[val_mask] = lgb_val_fold\n",
    "        xgb_oof[val_mask] = xgb_val_fold\n",
    "        knn_oof[val_mask] = knn_val_fold\n",
    "        \n",
    "        # Accumulate test predictions across folds\n",
    "        nn_preds += nn_test_fold / n_folds\n",
    "        lgb_preds += lgb_test_fold / n_folds\n",
    "        xgb_preds += xgb_test_fold / n_folds\n",
    "        knn_preds += knn_test_fold / n_folds\n",
    "        \n",
    "        # Store feature importance for this fold\n",
    "        if lgb_importance is not None:\n",
    "            fold_importance = lgb_importance.copy()\n",
    "            fold_importance['fold'] = fold + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "        \n",
    "        # Create a fold-level ensemble for validation\n",
    "        fold_oof_preds = [lgb_val_fold, xgb_val_fold, knn_val_fold, nn_val_fold]\n",
    "        fold_weights = [lgb_auc, xgb_auc, knn_auc, nn_auc]\n",
    "        \n",
    "        fold_blend = simple_weighted_ensemble(fold_oof_preds, fold_weights)\n",
    "        fold_auc = roc_auc_score(y_val, fold_blend)\n",
    "        fold_aucs.append(fold_auc)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Ensemble AUC: {fold_auc:.6f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "        if has_torch and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate overall OOF AUC for each model\n",
    "    nn_auc = roc_auc_score(target, nn_oof)\n",
    "    lgb_auc = roc_auc_score(target, lgb_oof)\n",
    "    xgb_auc = roc_auc_score(target, xgb_oof)\n",
    "    knn_auc = roc_auc_score(target, knn_oof)\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n=== Model Performance Summary ===\")\n",
    "    print(f\"Neural Network OOF AUC: {nn_auc:.6f}\")\n",
    "    print(f\"LightGBM OOF AUC: {lgb_auc:.6f}\")\n",
    "    print(f\"XGBoost OOF AUC: {xgb_auc:.6f}\")\n",
    "    print(f\"k-NN OOF AUC: {knn_auc:.6f}\")\n",
    "    print(f\"Mean Fold Ensemble AUC: {np.mean(fold_aucs):.6f}  {np.std(fold_aucs):.6f}\")\n",
    "    \n",
    "    # Calculate average feature importance\n",
    "    if not feature_importance.empty:\n",
    "        feature_importance_avg = (feature_importance.groupby('Feature')['Importance']\n",
    "                                .mean().reset_index().sort_values('Importance', ascending=False))\n",
    "        \n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        print(feature_importance_avg.head(15))\n",
    "    else:\n",
    "        feature_importance_avg = pd.DataFrame(columns=['Feature', 'Importance'])\n",
    "        print(\"\\nNo feature importance available (LightGBM not used).\")\n",
    "    \n",
    "    # Create and evaluate ensemble strategies\n",
    "    print(\"\\n=== Creating Final Ensemble ===\")\n",
    "    \n",
    "    # List of model predictions\n",
    "    predictions_list = [lgb_preds, xgb_preds, knn_preds, nn_preds]\n",
    "    oof_predictions_list = [lgb_oof, xgb_oof, knn_oof, nn_oof]\n",
    "    \n",
    "    # 1. Optimize weights using OOF predictions\n",
    "    optimal_weights, opt_auc = optimize_ensemble_weights(oof_predictions_list, target)\n",
    "    \n",
    "    # 2. Try weather-adaptive ensemble\n",
    "    try:\n",
    "        adaptive_preds = adaptive_weather_ensemble(\n",
    "            predictions_list, oof_predictions_list, target, features, test_features, base_path\n",
    "        )\n",
    "        \n",
    "        # Blend optimized and adaptive ensembles\n",
    "        final_preds = 0.6 * adaptive_preds + 0.4 * simple_weighted_ensemble(predictions_list, optimal_weights)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in adaptive ensemble: {str(e)}\")\n",
    "        print(\"Falling back to optimized weight ensemble.\")\n",
    "        final_preds = simple_weighted_ensemble(predictions_list, optimal_weights)\n",
    "    \n",
    "    # 3. Optional: Apply probability calibration\n",
    "    try:\n",
    "        calibrated_preds = calibrate_predictions(final_preds, method='isotonic', base_path=base_path)\n",
    "        \n",
    "        # Blend original and calibrated predictions (conservative approach)\n",
    "        final_preds = 0.7 * final_preds + 0.3 * calibrated_preds\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calibration: {str(e)}\")\n",
    "        print(\"Using uncalibrated ensemble predictions.\")\n",
    "    \n",
    "    return final_preds, feature_importance_avg\n",
    "\n",
    "def create_visualizations(predictions, feature_importance, features, target, test_features, base_path):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations with display options\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        feature_importance: Feature importance DataFrame\n",
    "        features: Features DataFrame\n",
    "        target: Target variable\n",
    "        test_features: Test features DataFrame\n",
    "        base_path: Base path for saving visualizations\n",
    "    \"\"\"\n",
    "    print(\"Creating detailed visualizations...\")\n",
    "    \n",
    "    # Create directory for visualizations in the same folder as data\n",
    "    viz_dir = os.path.join(base_path, 'visualizations')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Feature importance plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    top_features = feature_importance.head(20)\n",
    "    ax = sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
    "    # Add values on the plot\n",
    "    for i, v in enumerate(top_features['Importance']):\n",
    "        ax.text(v + 0.1, i, f\"{v:.1f}\", va='center')\n",
    "    plt.title('Top 20 Most Important Features', fontsize=16)\n",
    "    plt.xlabel('Importance Score', fontsize=14)\n",
    "    plt.ylabel('Feature', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    viz_path = os.path.join(viz_dir, 'feature_importance.png')\n",
    "    plt.savefig(viz_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Prediction distribution\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = sns.histplot(predictions, bins=50, kde=True, color='darkblue')\n",
    "    mean_val = np.mean(predictions)\n",
    "    median_val = np.median(predictions)\n",
    "    \n",
    "    # Add threshold and statistics lines\n",
    "    plt.axvline(x=0.5, color='red', linestyle='--', label='Classification Threshold (0.5)')\n",
    "    plt.axvline(x=mean_val, color='green', linestyle='-', label=f'Mean: {mean_val:.3f}')\n",
    "    plt.axvline(x=median_val, color='orange', linestyle=':', label=f'Median: {median_val:.3f}')\n",
    "    \n",
    "    # Add annotations\n",
    "    rain_pct = (predictions > 0.5).mean() * 100\n",
    "    plt.annotate(f\"Predicted rain: {rain_pct:.1f}%\", \n",
    "                 xy=(0.75, 0.9), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.title('Distribution of Rainfall Predictions', fontsize=16)\n",
    "    plt.xlabel('Predicted Probability', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    viz_path = os.path.join(viz_dir, 'prediction_distribution.png')\n",
    "    plt.savefig(viz_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Weather pattern analysis - only if clusters are available\n",
    "    if 'weather_cluster' in features.columns:\n",
    "        # Visualization setup\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.suptitle('Weather Pattern Analysis', fontsize=20, y=0.98)\n",
    "        \n",
    "        # Plot 1: Rainfall probability by cluster (top left)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        cluster_rain = pd.DataFrame({\n",
    "            'cluster': features['weather_cluster'],\n",
    "            'rainfall': target\n",
    "        }).groupby('cluster')['rainfall'].mean() * 100\n",
    "        \n",
    "        ax = sns.barplot(x=cluster_rain.index, y=cluster_rain.values, palette='Blues_d')\n",
    "        for i, p in enumerate(ax.patches):\n",
    "            ax.annotate(f\"{cluster_rain.values[i]:.1f}%\", \n",
    "                       (p.get_x() + p.get_width()/2., p.get_height() + 1), \n",
    "                       ha='center', va='bottom', fontsize=11)\n",
    "        \n",
    "        plt.title('Rainfall Probability by Weather Cluster', fontsize=16)\n",
    "        plt.xlabel('Weather Cluster', fontsize=14)\n",
    "        plt.ylabel('Probability of Rainfall (%)', fontsize=14)\n",
    "        plt.ylim(0, 100)  # Set fixed y-axis for better comparison\n",
    "        \n",
    "        # Plot 2: Key meteorological features by cluster (top right)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        key_features = ['cloud', 'humidity', temp_col, 'pressure', 'windspeed', 'sunshine']\n",
    "        cluster_data = features[key_features + ['weather_cluster']].groupby('weather_cluster').mean()\n",
    "        \n",
    "        # Scale for better visualization\n",
    "        scaler = StandardScaler()\n",
    "        cluster_data_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(cluster_data),\n",
    "            columns=cluster_data.columns,\n",
    "            index=cluster_data.index\n",
    "        )\n",
    "        \n",
    "        # Enhanced heatmap with better formatting\n",
    "        sns.heatmap(cluster_data_scaled, annot=True, cmap='coolwarm', fmt=\".2f\", \n",
    "                   linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Key Meteorological Features by Weather Cluster', fontsize=16)\n",
    "        \n",
    "        # Plot 3: Cloud-Humidity scatter with rainfall coloring (bottom left)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        scatter_data = pd.DataFrame({\n",
    "            'Cloud': features['cloud'],\n",
    "            'Humidity': features['humidity'],\n",
    "            'Rainfall': target\n",
    "        })\n",
    "        \n",
    "        # Add cluster information as marker size\n",
    "        if 'weather_cluster' in features.columns:\n",
    "            scatter_data['Cluster'] = features['weather_cluster']\n",
    "            \n",
    "        # Create scatter plot with color by rainfall and custom sizing\n",
    "        scatter = plt.scatter(\n",
    "            scatter_data['Cloud'], \n",
    "            scatter_data['Humidity'],\n",
    "            c=scatter_data['Rainfall'], \n",
    "            cmap='coolwarm',\n",
    "            alpha=0.6, \n",
    "            s=50\n",
    "        )\n",
    "        \n",
    "        # Add contour lines to show regions of similar rainfall probability\n",
    "        try:\n",
    "            from scipy.interpolate import griddata\n",
    "            from matplotlib.colors import LinearSegmentedColormap\n",
    "            \n",
    "            x = scatter_data['Cloud']\n",
    "            y = scatter_data['Humidity']\n",
    "            z = scatter_data['Rainfall']\n",
    "            \n",
    "            # Create grid for contour\n",
    "            xi = np.linspace(x.min(), x.max(), 100)\n",
    "            yi = np.linspace(y.min(), y.max(), 100)\n",
    "            xi, yi = np.meshgrid(xi, yi)\n",
    "            \n",
    "            # Interpolate\n",
    "            zi = griddata((x, y), z, (xi, yi), method='linear')\n",
    "            \n",
    "            # Create contour\n",
    "            contour = plt.contour(xi, yi, zi, levels=[0.25, 0.5, 0.75], colors='black', alpha=0.7)\n",
    "            plt.clabel(contour, inline=True, fontsize=8)\n",
    "            \n",
    "            # Add legend for contour\n",
    "            custom_lines = [plt.Line2D([0], [0], color='black', lw=1),\n",
    "                           plt.Line2D([0], [0], color='black', lw=1, ls='--')]\n",
    "            plt.legend(custom_lines, ['Rainfall probability contours', '50% probability line'], \n",
    "                      loc='lower right')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create contour plot: {e}\")\n",
    "        \n",
    "        plt.title('Cloud-Humidity Relationship with Rainfall', fontsize=16)\n",
    "        plt.xlabel('Cloud Cover', fontsize=14)\n",
    "        plt.ylabel('Humidity', fontsize=14)\n",
    "        plt.colorbar(scatter, label='Rainfall Occurrence')\n",
    "        \n",
    "        # Plot 4: Feature correlations focused on the most important ones (bottom right)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        top_feature_names = list(feature_importance.head(8)['Feature'])\n",
    "        # Add rainfall to correlations\n",
    "        features_subset = features.copy()\n",
    "        features_subset['rainfall'] = target\n",
    "        correlation_subset = features_subset[top_feature_names + ['rainfall']]\n",
    "        \n",
    "        # Enhanced correlation matrix\n",
    "        corr_matrix = correlation_subset.corr()\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "        sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                   annot=True, fmt='.2f', square=True, linewidths=.5)\n",
    "        \n",
    "        plt.title('Correlation Matrix of Top Features with Rainfall', fontsize=16)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for suptitle\n",
    "        viz_path = os.path.join(viz_dir, 'weather_pattern_analysis.png')\n",
    "        plt.savefig(viz_path, dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Model performance comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Example model performance data - will be replaced with actual data when available\n",
    "    try:\n",
    "        models = ['LightGBM', 'XGBoost', 'k-NN', 'Neural Network', 'Ensemble']\n",
    "        aucs = [lgb_auc, xgb_auc, knn_auc, nn_auc, np.mean(fold_aucs)]\n",
    "    except:\n",
    "        # Fallback sample data if actual not available\n",
    "        models = ['LightGBM', 'XGBoost', 'k-NN', 'Neural Network', 'Ensemble']\n",
    "        aucs = [0.838, 0.869, 0.86, 0.885, 0.887]\n",
    "    \n",
    "    # Calculate model variance\n",
    "    std_auc = np.std(aucs[:-1])\n",
    "    \n",
    "    # Create model comparison plot\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "    ax = sns.barplot(x=models, y=aucs, palette=colors)\n",
    "    \n",
    "    # Add AUC values on bars\n",
    "    for i, v in enumerate(aucs):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add baseline\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random Guess (AUC=0.5)')\n",
    "    \n",
    "    # Add metrics summary\n",
    "    plt.annotate(f\"Ensemble improvement: +{(aucs[-1] - max(aucs[:-1]))*100:.1f}% over best model\\nModel std. dev: {std_auc:.3f}\", \n",
    "                xy=(0.02, 0.02), xycoords='axes fraction', bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.title('Model Performance Comparison (AUC)', fontsize=16)\n",
    "    plt.ylabel('AUC Score', fontsize=14)\n",
    "    plt.ylim(0.7, 1.0)  # Set y-axis range for better visualization of differences\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    viz_path = os.path.join(viz_dir, 'model_comparison.png')\n",
    "    plt.savefig(viz_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Feature effect analysis\n",
    "    try:\n",
    "        print(\"Creating feature effect visualizations...\")\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        top_features = feature_importance.head(6)['Feature'].tolist()\n",
    "        for i, feature in enumerate(top_features):\n",
    "            if feature in features.columns:\n",
    "                plt.subplot(2, 3, i+1)\n",
    "                \n",
    "                # Sort feature values and corresponding target\n",
    "                feature_df = pd.DataFrame({\n",
    "                    'feature': features[feature],\n",
    "                    'target': target\n",
    "                }).sort_values('feature')\n",
    "                \n",
    "                # Smooth the relationship using moving average\n",
    "                window = min(100, len(feature_df) // 10)\n",
    "                feature_df['rolling_mean'] = feature_df['target'].rolling(window=window, center=True).mean()\n",
    "                \n",
    "                # Plot both scatter and trend\n",
    "                sns.scatterplot(x='feature', y='target', data=feature_df, alpha=0.2, s=10)\n",
    "                sns.lineplot(x='feature', y='rolling_mean', data=feature_df, color='red', linewidth=2)\n",
    "                \n",
    "                plt.title(f'Effect of {feature} on Rainfall', fontsize=14)\n",
    "                plt.xlabel(feature, fontsize=12)\n",
    "                plt.ylabel('Rainfall Probability', fontsize=12)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        viz_path = os.path.join(viz_dir, 'feature_effects.png')\n",
    "        plt.savefig(viz_path, dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create feature effect plots: {e}\")\n",
    "    \n",
    "    print(f\"\\nVisualizations have been saved to: {viz_dir}\")\n",
    "    print(\"Visualization files:\")\n",
    "    viz_files = ['feature_importance.png', 'prediction_distribution.png', \n",
    "                'weather_pattern_analysis.png', 'model_comparison.png', \n",
    "                'feature_effects.png']\n",
    "    for viz_file in viz_files:\n",
    "        viz_path = os.path.join(viz_dir, viz_file)\n",
    "        if os.path.exists(viz_path):\n",
    "            print(f\" - {viz_file}\")\n",
    "    \n",
    "    # Return paths to visualizations for reference\n",
    "    return {\n",
    "        'feature_importance': os.path.join(viz_dir, 'feature_importance.png'),\n",
    "        'prediction_distribution': os.path.join(viz_dir, 'prediction_distribution.png'),\n",
    "        'weather_patterns': os.path.join(viz_dir, 'weather_pattern_analysis.png'),\n",
    "        'model_comparison': os.path.join(viz_dir, 'model_comparison.png'),\n",
    "        'feature_effects': os.path.join(viz_dir, 'feature_effects.png')\n",
    "    }\n",
    "\n",
    "def analyze_errors(predictions, target, features, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Analyze model prediction errors to understand strengths and weaknesses\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (probabilities)\n",
    "        target: Actual target values (0/1)\n",
    "        features: Features DataFrame\n",
    "        threshold: Classification threshold (default: 0.5)\n",
    "    \"\"\"\n",
    "    # Create binary predictions using threshold\n",
    "    binary_preds = (predictions > threshold).astype(int)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    error_df = pd.DataFrame({\n",
    "        'actual': target,\n",
    "        'predicted': binary_preds,\n",
    "        'probability': predictions\n",
    "    })\n",
    "    \n",
    "    # Add key features\n",
    "    for col in ['cloud', 'humidity', 'cloud_humidity_product', 'weighted_cloud_humidity']:\n",
    "        if col in features.columns:\n",
    "            error_df[col] = features[col].values\n",
    "    \n",
    "    # Add weather cluster if available\n",
    "    if 'weather_cluster' in features.columns:\n",
    "        error_df['weather_cluster'] = features['weather_cluster'].values\n",
    "    \n",
    "    # Calculate error types\n",
    "    error_df['correct'] = (error_df['actual'] == error_df['predicted']).astype(int)\n",
    "    error_df['false_positive'] = ((error_df['actual'] == 0) & (error_df['predicted'] == 1)).astype(int)\n",
    "    error_df['false_negative'] = ((error_df['actual'] == 1) & (error_df['predicted'] == 0)).astype(int)\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = (error_df['correct'].sum() / len(error_df)) * 100\n",
    "    false_positive_rate = error_df['false_positive'].mean() * 100\n",
    "    false_negative_rate = error_df['false_negative'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n=== Error Analysis ===\")\n",
    "    print(f\"Overall accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"False positive rate: {false_positive_rate:.2f}%\")\n",
    "    print(f\"False negative rate: {false_negative_rate:.2f}%\")\n",
    "    \n",
    "    # Performance by weather cluster\n",
    "    if 'weather_cluster' in error_df.columns:\n",
    "        print(\"\\nAccuracy by weather cluster:\")\n",
    "        cluster_accuracy = error_df.groupby('weather_cluster')['correct'].mean() * 100\n",
    "        for cluster, acc in cluster_accuracy.items():\n",
    "            print(f\"Cluster {cluster}: {acc:.2f}%\")\n",
    "    \n",
    "    # Create visualizations (only if we have a base_path)\n",
    "    try:\n",
    "        # Find the base path from features path\n",
    "        base_path = os.getcwd()  # Default to current directory\n",
    "        \n",
    "        # Create directory for visualizations\n",
    "        viz_dir = os.path.join(base_path, 'visualizations')\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Error distribution by probability\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        \n",
    "        # Show distribution of correct and incorrect predictions\n",
    "        sns.histplot(\n",
    "            data=error_df, x='probability', hue='correct', \n",
    "            bins=25, alpha=0.6, palette={0: 'red', 1: 'green'},\n",
    "            multiple='stack'\n",
    "        )\n",
    "        plt.axvline(x=threshold, color='black', linestyle='--', label=f'Threshold: {threshold}')\n",
    "        plt.title('Prediction Distribution by Correctness', fontsize=14)\n",
    "        plt.xlabel('Predicted Probability', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.legend(['Threshold', 'Incorrect', 'Correct'])\n",
    "        \n",
    "        # 2. Cloud vs Humidity plot colored by error type\n",
    "        plt.subplot(2, 2, 2)\n",
    "        \n",
    "        # Create a color map for different error types\n",
    "        error_df['error_type'] = 'Correct'\n",
    "        error_df.loc[error_df['false_positive'] == 1, 'error_type'] = 'False Positive'\n",
    "        error_df.loc[error_df['false_negative'] == 1, 'error_type'] = 'False Negative'\n",
    "        \n",
    "        # Create unique color map\n",
    "        error_colors = {'Correct': 'green', 'False Positive': 'orange', 'False Negative': 'red'}\n",
    "        \n",
    "        # Plot scatter\n",
    "        sns.scatterplot(\n",
    "            data=error_df, x='cloud', y='humidity', hue='error_type',\n",
    "            palette=error_colors, alpha=0.7, s=50\n",
    "        )\n",
    "        plt.title('Cloud vs Humidity by Error Type', fontsize=14)\n",
    "        plt.xlabel('Cloud Cover', fontsize=12)\n",
    "        plt.ylabel('Humidity', fontsize=12)\n",
    "        \n",
    "        # 3. Feature values for the most challenging cases\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        # Find the most uncertain predictions (near 0.5)\n",
    "        uncertain_preds = error_df[\n",
    "            (error_df['probability'] > 0.4) & \n",
    "            (error_df['probability'] < 0.6)\n",
    "        ]\n",
    "        \n",
    "        # Calculate accuracy in different feature regions\n",
    "        if 'cloud_humidity_product' in error_df.columns:\n",
    "            error_df['ch_product_bins'] = pd.qcut(error_df['cloud_humidity_product'], 5, duplicates='drop')\n",
    "            accuracy_by_ch = error_df.groupby('ch_product_bins')['correct'].mean() * 100\n",
    "            \n",
    "            # Plot accuracy by cloud-humidity product\n",
    "            accuracy_by_ch.plot(kind='bar')\n",
    "            plt.title('Accuracy by Cloud-Humidity Product', fontsize=14)\n",
    "            plt.xlabel('Cloud-Humidity Product Range', fontsize=12)\n",
    "            plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # 4. Most confused weather clusters\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        if 'weather_cluster' in error_df.columns:\n",
    "            # Calculate error rates by cluster\n",
    "            cluster_stats = error_df.groupby('weather_cluster').agg({\n",
    "                'correct': 'mean',\n",
    "                'false_positive': 'mean',\n",
    "                'false_negative': 'mean'\n",
    "            }) * 100\n",
    "            \n",
    "            # Reshape for plotting\n",
    "            cluster_stats = cluster_stats.reset_index()\n",
    "            cluster_stats_melted = pd.melt(\n",
    "                cluster_stats, id_vars='weather_cluster',\n",
    "                value_vars=['correct', 'false_positive', 'false_negative'],\n",
    "                var_name='Metric', value_name='Percentage'\n",
    "            )\n",
    "            \n",
    "            # Rename for better labels\n",
    "            cluster_stats_melted['Metric'] = cluster_stats_melted['Metric'].replace({\n",
    "                'correct': 'Accuracy',\n",
    "                'false_positive': 'False Positive',\n",
    "                'false_negative': 'False Negative'\n",
    "            })\n",
    "            \n",
    "            # Plot\n",
    "            sns.barplot(\n",
    "                data=cluster_stats_melted[cluster_stats_melted['Metric'] != 'Accuracy'], \n",
    "                x='weather_cluster', y='Percentage', hue='Metric',\n",
    "                palette={'False Positive': 'orange', 'False Negative': 'red'}\n",
    "            )\n",
    "            plt.title('Error Rates by Weather Cluster', fontsize=14)\n",
    "            plt.xlabel('Weather Cluster', fontsize=12)\n",
    "            plt.ylabel('Error Rate (%)', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        viz_path = os.path.join(viz_dir, 'error_analysis.png')\n",
    "        plt.savefig(viz_path, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Error analysis visualization saved to: {viz_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create error analysis visualization: {e}\")\n",
    "    \n",
    "    # Find the most challenging prediction cases\n",
    "    print(\"\\nMost challenging prediction cases:\")\n",
    "    \n",
    "    # Sort by prediction uncertainty (closest to 0.5)\n",
    "    error_df['uncertainty'] = abs(error_df['probability'] - 0.5)\n",
    "    uncertain_errors = error_df[error_df['correct'] == 0].sort_values('uncertainty').head(5)\n",
    "    \n",
    "    print(\"Most uncertain incorrect predictions:\")\n",
    "    print(uncertain_errors[['actual', 'probability', 'cloud', 'humidity', 'cloud_humidity_product']])\n",
    "    \n",
    "    return error_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the end-to-end rainfall prediction pipeline\"\"\"\n",
    "    print(\"=== Advanced Rainfall Prediction Model ===\")\n",
    "    print(\"Adapted for local file paths\\n\")\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = pd.Timestamp.now()\n",
    "    print(f\"Start time: {start_time}\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n[1/5] Loading data...\")\n",
    "    train, test, submission, base_path = load_data(\"C:/Nauka/Koo naukowe/Binary Prediction with a Rainfall Dataset\")\n",
    "    \n",
    "    # 2. Preprocess data with advanced feature engineering\n",
    "    print(\"\\n[2/5] Preprocessing data and creating meteorological features...\")\n",
    "    features, test_features, target, train_ids, test_ids = preprocess_data(\n",
    "        train, test, target_col='rainfall', id_col='id'\n",
    "    )\n",
    "    \n",
    "    # 3. Run cross-validation with multiple models\n",
    "    print(\"\\n[3/5] Running cross-validation with ensemble of models...\")\n",
    "    final_predictions, feature_importance = run_cross_validation(\n",
    "        features, target, test_features, base_path, n_folds=5\n",
    "    )\n",
    "    \n",
    "    # 4. Create and display visualizations\n",
    "    print(\"\\n[4/5] Creating detailed visualizations for analysis...\")\n",
    "    viz_paths = create_visualizations(final_predictions, feature_importance, features, target, test_features, base_path)\n",
    "    \n",
    "    # 5. Create submission file\n",
    "    print(\"\\n[5/5] Creating submission file...\")\n",
    "    if submission is None:\n",
    "        submission = pd.DataFrame({'id': test_ids, 'rainfall': final_predictions})\n",
    "    else:\n",
    "        submission['rainfall'] = final_predictions\n",
    "    \n",
    "    # Output submission to the same directory as input data\n",
    "    submission_path = os.path.join(base_path, 'rainfall_predictions.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission file saved to: {submission_path}\")\n",
    "    \n",
    "    # Optional: Error analysis on validation results\n",
    "    try:\n",
    "        # Use a simple 80/20 train/validation split for demonstration\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            features.drop('fold', axis=1), target, test_size=0.2, random_state=SEED, stratify=target\n",
    "        )\n",
    "        \n",
    "        # Train a simple model for validation predictions\n",
    "        if has_lightgbm:\n",
    "            lgb_params = get_lgb_params()\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            model = lgb.train(lgb_params, train_data, num_boost_round=500)\n",
    "            val_preds = model.predict(X_val)\n",
    "            # Perform error analysis\n",
    "            error_analysis = analyze_errors(val_preds, y_val, X_val)\n",
    "            print(\"\\nError analysis completed. Check visualization in the same folder.\")\n",
    "        else:\n",
    "            print(\"\\nLightGBM not available. Skipping error analysis.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform error analysis: {e}\")\n",
    "    \n",
    "    # Finish time and summary\n",
    "    end_time = pd.Timestamp.now()\n",
    "    duration = (end_time - start_time).total_seconds() / 60\n",
    "    \n",
    "    # Display submission summary\n",
    "    print(\"\\n=== Submission Summary ===\")\n",
    "    print(f\"First 5 rows of submission file:\")\n",
    "    print(submission.head())\n",
    "    print(f\"\\nPrediction statistics:\")\n",
    "    print(f\"Min: {final_predictions.min():.4f}, Max: {final_predictions.max():.4f}\")\n",
    "    print(f\"Mean: {final_predictions.mean():.4f}, Std: {final_predictions.std():.4f}\")\n",
    "    print(f\"Median: {np.median(final_predictions):.4f}\")\n",
    "    print(f\"\\nRun time: {duration:.2f} minutes\")\n",
    "    \n",
    "    print(\"\\n=== Model Training Complete ===\")\n",
    "    print(\"This optimized ensemble model incorporates:\")\n",
    "    print(\"- Meteorological feature engineering based on physical rainfall processes\")\n",
    "    print(\"- Weather pattern clustering with adaptive ensemble techniques\")\n",
    "    print(\"- Multiple complementary models (LightGBM, XGBoost, k-NN, Neural Network)\")\n",
    "    print(\"- Comprehensive visualization and model interpretation\")\n",
    "    \n",
    "    print(f\"\\nAll results and visualizations saved to: {base_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set larger figure size globally for better visualizations\n",
    "    plt.rcParams['figure.figsize'] = [12, 8]\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # Execute main pipeline\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b5f24-e951-4a00-b5db-3988eaf45a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
